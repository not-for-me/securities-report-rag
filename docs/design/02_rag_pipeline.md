# 02. RAG Pipeline ì„¤ê³„

> ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ PDFë¥¼ íŒŒì‹±í•˜ê³ , ì²­í‚¹ ë° ì„ë² ë”©ì„ ê±°ì³ Vector DBì— ì ì¬í•œ ë’¤, ìì—°ì–´ ì§ˆì˜ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” RAG íŒŒì´í”„ë¼ì¸ì˜ ìƒì„¸ ì„¤ê³„

---

## 1. PDF íŒŒì‹± ì „ëµ

### 1.1 Upstage Document Parse API í™œìš©

ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ëŠ” í…ìŠ¤íŠ¸, í…Œì´ë¸”, ì°¨íŠ¸, ì´ë¯¸ì§€ê°€ ë³µí•©ì ìœ¼ë¡œ êµ¬ì„±ëœ ë¬¸ì„œì´ë‹¤. Upstage Document Parse APIëŠ” ë ˆì´ì•„ì›ƒ ì¸ì‹ ê¸°ë°˜ìœ¼ë¡œ ì´ëŸ¬í•œ ë³µí•© ìš”ì†Œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ë° ê°•ì ì´ ìˆë‹¤.

**API í˜¸ì¶œ ê¸°ë³¸ êµ¬ì„±:**

```python
import httpx

async def parse_pdf(file_path: str) -> dict:
    url = "https://api.upstage.ai/v1/document-ai/document-parse"
    headers = {"Authorization": f"Bearer {UPSTAGE_API_KEY}"}

    with open(file_path, "rb") as f:
        files = {"document": f}
        data = {
            "output_format": "markdown",
            "coordinates": False,
            "ocr": "auto",
            "model": "document-parse",
        }
        response = httpx.post(url, headers=headers, files=files, data=data)
    return response.json()
```

### 1.2 output_format ì„ íƒ: Markdown

| ê¸°ì¤€ | Markdown | HTML |
|------|----------|------|
| ì²­í‚¹ í˜¸í™˜ì„± | LangChain `MarkdownHeaderTextSplitter`ì™€ ì§ì ‘ í˜¸í™˜ | ë³„ë„ íŒŒì„œ í•„ìš” |
| í…Œì´ë¸” í‘œí˜„ | Markdown í…Œì´ë¸” (íŒŒì´í”„ êµ¬ë¶„) | `<table>` íƒœê·¸ |
| í›„ì²˜ë¦¬ ìš©ì´ì„± | ì •ê·œì‹ìœ¼ë¡œ í—¤ë”/í…Œì´ë¸” íŒ¨í„´ ì¶”ì¶œ ìš©ì´ | DOM íŒŒì‹± í•„ìš” |
| LLM ì…ë ¥ ì í•©ì„± | LLMì´ Markdown í…Œì´ë¸”ì„ ì˜ ì´í•´ | HTMLë„ ê°€ëŠ¥í•˜ë‚˜ í† í° íš¨ìœ¨ ë‚®ìŒ |
| ê°€ë…ì„± | ì‚¬ëŒì´ ì§ì ‘ ì½ê¸° í¸í•¨ | íƒœê·¸ë¡œ ì¸í•´ ê°€ë…ì„± ì €í•˜ |

**ì„ íƒ: Markdown**

- LangChainì˜ Markdown ê¸°ë°˜ splitterì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê³„
- LLM ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë‚´ í† í° íš¨ìœ¨ì´ ë” ë†’ìŒ
- ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ì˜ í•µì‹¬ì¸ í…Œì´ë¸”ì´ Markdown í…Œì´ë¸”ë¡œ ê¹”ë”í•˜ê²Œ í‘œí˜„ë¨

### 1.3 ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ íŠ¹ì„±ì— ë§ëŠ” íŒŒì‹± ì˜µì…˜

```python
PARSE_OPTIONS = {
    "output_format": "markdown",
    "coordinates": False,       # ì¢Œí‘œ ì •ë³´ ë¶ˆí•„ìš” (í…ìŠ¤íŠ¸ ê²€ìƒ‰ ëª©ì )
    "ocr": "auto",             # ìŠ¤ìº” PDFê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ auto
    "model": "document-parse",  # ìµœì‹  ëª¨ë¸ ì‚¬ìš©
}
```

**ì£¼ì˜ì‚¬í•­:**
- ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ëŠ” ëŒ€ë¶€ë¶„ ë””ì§€í„¸ PDFì´ë‚˜, ê°„í˜¹ ìŠ¤ìº” ì´ë¯¸ì§€ê°€ í¬í•¨ë˜ë¯€ë¡œ `ocr: auto` ì„¤ì •
- ì°¨íŠ¸/ê·¸ë˜í”„ ì´ë¯¸ì§€ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ë¶ˆê°€í•˜ë¯€ë¡œ, ì°¨íŠ¸ ì•„ë˜ ìº¡ì…˜ í…ìŠ¤íŠ¸ë¥¼ í™œìš©
- íŒŒì‹± ê²°ê³¼ëŠ” `data/parsed/{íŒŒì¼ëª…}.md`ë¡œ ìºì‹±í•˜ì—¬ ì¬íŒŒì‹± ë°©ì§€

---

## 2. ì²­í‚¹ ì „ëµ ìƒì„¸

### 2.1 ì²­í‚¹ ì „ëµ ê°œìš”

ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ëŠ” í…Œì´ë¸”ê³¼ í…ìŠ¤íŠ¸ê°€ í˜¼ì¬í•˜ë©°, ê°ê° ë‹¤ë¥¸ ì²­í‚¹ ì „ëµì´ í•„ìš”í•˜ë‹¤.

```
[ë¦¬í¬íŠ¸ íŒŒì‹± ê²°ê³¼]
    â”‚
    â”œâ”€ ë””ìŠ¤í´ë ˆì´ë¨¸ í•„í„°ë§ (ì œê±°)
    â”‚
    â”œâ”€ í…Œì´ë¸” ì²­í¬ ì¶”ì¶œ (í…Œì´ë¸” + ì „í›„ ì„¤ëª…)
    â”‚
    â””â”€ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  (ì„¹ì…˜/ì†Œì œëª© ê¸°ì¤€)
         â”‚
         â””â”€ ìµœì¢… ì²­í¬ ë¦¬ìŠ¤íŠ¸ + ë©”íƒ€ë°ì´í„°
```

### 2.2 í…Œì´ë¸” ì²­í¬

í…Œì´ë¸”ì€ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ì˜ í•µì‹¬ ì •ë³´(ì‹¤ì  ì¶”ì •, ë°¸ë¥˜ì—ì´ì…˜, íˆ¬ìì˜ê²¬ ë“±)ë¥¼ ë‹´ê³  ìˆìœ¼ë¯€ë¡œ ì ˆëŒ€ ë¶„í• í•˜ì§€ ì•ŠëŠ”ë‹¤.

**ê·œì¹™:**

1. **í…Œì´ë¸” íƒì§€**: Markdown í…Œì´ë¸” íŒ¨í„´(`|---|` í¬í•¨ ë¸”ë¡)ì„ ì •ê·œì‹ìœ¼ë¡œ íƒì§€
2. **í…Œì´ë¸” ë³´ì¡´**: í…Œì´ë¸” ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ìœ ì§€
3. **ì»¨í…ìŠ¤íŠ¸ í¬í•¨**: í…Œì´ë¸” ì§ì „ 1~2 ë¬¸ë‹¨(ì„¤ëª…)ê³¼ ì§í›„ 1 ë¬¸ë‹¨(í•´ì„)ì„ ê°™ì€ ì²­í¬ì— í¬í•¨
4. **ì²­í¬ íƒ€ì… íƒœê¹…**: `chunk_type: "table"` ë©”íƒ€ë°ì´í„° ë¶€ì°©

```python
import re

TABLE_PATTERN = re.compile(
    r"((?:^.*\n)?)"           # í…Œì´ë¸” ì§ì „ ë¬¸ë‹¨ (ì„ íƒ)
    r"((?:^\|.+\|$\n?)+"     # í…Œì´ë¸” ë³¸ì²´ (íŒŒì´í”„ë¡œ ì‹œì‘/ë)
    r"(?:^\|[-:| ]+\|$\n?)"  # êµ¬ë¶„ì„ 
    r"(?:^\|.+\|$\n?)*)"     # ë°ì´í„° í–‰
    r"((?:^.*\n)?)",          # í…Œì´ë¸” ì§í›„ ë¬¸ë‹¨ (ì„ íƒ)
    re.MULTILINE
)
```

**í…Œì´ë¸” ì „í›„ ë¬¸ë‹¨ í¬í•¨ ê¸°ì¤€:**
- ì§ì „ ë¬¸ë‹¨: í…Œì´ë¸” ë°”ë¡œ ìœ„ì˜ ë¹„ì–´ ìˆì§€ ì•Šì€ í…ìŠ¤íŠ¸ ì¤„ (ë³´í†µ í…Œì´ë¸” ì œëª©/ì„¤ëª…)
- ì§í›„ ë¬¸ë‹¨: í…Œì´ë¸” ë°”ë¡œ ì•„ë˜ì˜ ë¹„ì–´ ìˆì§€ ì•Šì€ í…ìŠ¤íŠ¸ ì¤„ (ë³´í†µ ì¶œì²˜ ë˜ëŠ” ì£¼ì„)
- ì§ì „/ì§í›„ í…ìŠ¤íŠ¸ê°€ ë‹¤ë¥¸ í…Œì´ë¸”ì´ë©´ í¬í•¨í•˜ì§€ ì•ŠìŒ

### 2.3 í…ìŠ¤íŠ¸ ì²­í¬

**ë¶„í•  ê¸°ì¤€:**

1. **1ì°¨ ë¶„í• **: Markdown í—¤ë”(`#`, `##`, `###`) ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„í• 
2. **2ì°¨ ë¶„í• **: ì„¹ì…˜ì´ ë„ˆë¬´ ê¸¸ ê²½ìš° `RecursiveCharacterTextSplitter`ë¡œ ì¶”ê°€ ë¶„í• 

**ì²­í¬ í¬ê¸° ì„¤ì •:**

| íŒŒë¼ë¯¸í„° | ê°’ | ê·¼ê±° |
|----------|-----|------|
| `chunk_size` | 1,000 tokens (ì•½ 2,000~2,500 í•œê¸€ ë¬¸ì) | ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í•œ ì„¹ì…˜ì˜ í‰ê·  ê¸¸ì´ì— ê·¼ì ‘. `text-embedding-3-small`ì˜ ìµœëŒ€ ì…ë ¥ 8,191 tokens ì´ë‚´ì—ì„œ ì¶©ë¶„í•œ ì˜ë¯¸ ë‹¨ìœ„ í™•ë³´ |
| `chunk_overlap` | 200 tokens (ì•½ 400~500 í•œê¸€ ë¬¸ì) | ì„¹ì…˜ ê²½ê³„ì—ì„œ ë¬¸ë§¥ ì†ì‹¤ ë°©ì§€. ì „ì²´ ì²­í¬ ëŒ€ë¹„ 20% ìˆ˜ì¤€ìœ¼ë¡œ ì ì ˆ |
| `separators` | `["\n## ", "\n### ", "\n\n", "\n", " "]` | Markdown í—¤ë” ìš°ì„ , ë¬¸ë‹¨ ê²½ê³„ ì°¨ì„  |

### 2.4 Splitter ë¹„êµ ë° ì„ íƒ

#### MarkdownHeaderTextSplitter

- **ì¥ì **: Markdown í—¤ë” ê³„ì¸µì„ ì¸ì‹í•˜ì—¬ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì •í™•í•˜ê²Œ ë¶„í• . í—¤ë”ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ìë™ ì¶”ì¶œ
- **ë‹¨ì **: í—¤ë”ê°€ ì—†ëŠ” ê¸´ ë³¸ë¬¸ì€ ë¶„í• í•˜ì§€ ëª»í•¨. í…Œì´ë¸” ë‚´ë¶€ ë¶„í•  ìœ„í—˜

#### RecursiveCharacterTextSplitter

- **ì¥ì **: ë²”ìš©ì . chunk_size/overlap ì„¸ë°€ ì œì–´ ê°€ëŠ¥. ê¸´ í…ìŠ¤íŠ¸ë„ ì•ˆì •ì ìœ¼ë¡œ ë¶„í• 
- **ë‹¨ì **: ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¥ì´ ì–´ë ¤ì›€. ë¬¸ì¥ ì¤‘ê°„ ì ˆë‹¨ ê°€ëŠ¥ì„±

#### ì„ íƒ: 2ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼

```python
from langchain.text_splitter import (
    MarkdownHeaderTextSplitter,
    RecursiveCharacterTextSplitter,
)

# 1ë‹¨ê³„: Markdown í—¤ë” ê¸°ì¤€ ë¶„í• 
headers_to_split_on = [
    ("#", "h1"),
    ("##", "h2"),
    ("###", "h3"),
]
markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on,
    strip_headers=False,  # í—¤ë”ë¥¼ ì²­í¬ ë‚´ìš©ì— ìœ ì§€ (ê²€ìƒ‰ ì‹œ ì»¨í…ìŠ¤íŠ¸)
)

# 2ë‹¨ê³„: í° ì„¹ì…˜ì„ ì¶”ê°€ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " "],
    length_function=len,  # ì‹¤ì œ êµ¬í˜„ ì‹œ tiktoken ê¸°ë°˜ í† í° ì¹´ìš´í„° ì‚¬ìš©
)

def split_text_chunks(markdown_text: str) -> list:
    # 1ë‹¨ê³„: í—¤ë” ê¸°ì¤€ ë¶„í• 
    header_splits = markdown_splitter.split_text(markdown_text)

    # 2ë‹¨ê³„: í° ì²­í¬ ì¶”ê°€ ë¶„í• 
    final_chunks = text_splitter.split_documents(header_splits)
    return final_chunks
```

**í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ì˜ ì´ì :**
- `MarkdownHeaderTextSplitter`ë¡œ ì˜ë¯¸ ë‹¨ìœ„ë¥¼ ë¨¼ì € í™•ë³´
- ì˜ë¯¸ ë‹¨ìœ„ê°€ ë„ˆë¬´ ê¸´ ê²½ìš°ì—ë§Œ `RecursiveCharacterTextSplitter`ê°€ ë³´ì¡°ì ìœ¼ë¡œ ë¶„í• 
- í—¤ë” ì •ë³´ê°€ ë©”íƒ€ë°ì´í„°ë¡œ ìë™ ë³´ì¡´ë˜ì–´ retrieval ì‹œ ì„¹ì…˜ ì»¨í…ìŠ¤íŠ¸ ì œê³µ

### 2.5 ë””ìŠ¤í´ë ˆì´ë¨¸ í•„í„°ë§

ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ í•˜ë‹¨ì—ëŠ” ë²•ì  ê³ ì§€ì‚¬í•­(ë””ìŠ¤í´ë ˆì´ë¨¸)ì´ í¬í•¨ë˜ë©°, ì´ëŠ” ê²€ìƒ‰/ë‹µë³€ì— ë¶ˆí•„ìš”í•˜ë‹¤.

**í•„í„°ë§ ì „ëµ:**

```python
DISCLAIMER_PATTERNS = [
    r"ë³¸\s*ì¡°ì‚¬ìë£ŒëŠ”\s*ê³ ê°ì˜\s*íˆ¬ìì—\s*ì°¸ê³ ",
    r"íˆ¬ìíŒë‹¨ì˜\s*ìµœì¢…\s*ì±…ì„ì€",
    r"ë‹¹ì‚¬ëŠ”\s*ë³¸\s*ìë£Œì˜\s*ë‚´ìš©ì—\s*ì˜ê±°í•˜ì—¬",
    r"ì´\s*ìë£Œì—\s*ê²Œì¬ëœ\s*ë‚´ìš©ë“¤ì€\s*ì‘ì„±ìì˜\s*ì˜ê²¬",
    r"Compliance\s*Notice",
    r"ë³¸\s*ìë£ŒëŠ”\s*íˆ¬ì\s*ì°¸ê³ ìš©ìœ¼ë¡œ\s*ì‘ì„±",
    r"ê³¼ê±°ì˜\s*ìˆ˜ìµë¥ .*ë¯¸ë˜ì˜\s*ìˆ˜ìµë¥ ì„\s*ë³´ì¥",
]

def filter_disclaimers(chunks: list) -> list:
    """ë””ìŠ¤í´ë ˆì´ë¨¸ íŒ¨í„´ì´ í¬í•¨ëœ ì²­í¬ë¥¼ ì œê±°"""
    filtered = []
    for chunk in chunks:
        content = chunk.page_content
        is_disclaimer = any(
            re.search(pattern, content) for pattern in DISCLAIMER_PATTERNS
        )
        if not is_disclaimer:
            filtered.append(chunk)
    return filtered
```

**í•„í„°ë§ ì‹œì **: ì²­í‚¹ ì™„ë£Œ í›„, ì„ë² ë”© ì „ì— ìˆ˜í–‰

---

## 3. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

### 3.1 ì¶”ì¶œ ëŒ€ìƒ

| í•„ë“œ | íƒ€ì… | ì„¤ëª… | ì¶”ì¶œ ë°©ë²• |
|------|------|------|-----------|
| `ticker` | string | ì¢…ëª©ì½”ë“œ (ì˜ˆ: 005930) | ì •ê·œì‹ + ì¢…ëª© ì½”ë“œ ë§¤í•‘ í…Œì´ë¸” |
| `company_name` | string | ì¢…ëª©ëª… (ì˜ˆ: ì‚¼ì„±ì „ì) | íŒŒì‹± ê²°ê³¼ ì²« í˜ì´ì§€ì—ì„œ ì¶”ì¶œ |
| `date` | string | ë¦¬í¬íŠ¸ ë°œí–‰ì¼ (YYYY-MM-DD) | ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ |
| `broker` | string | ì¦ê¶Œì‚¬ëª… | íŒŒì¼ëª… ê·œì¹™ ë˜ëŠ” ë³¸ë¬¸ íŒ¨í„´ |
| `analyst` | string | ì• ë„ë¦¬ìŠ¤íŠ¸ëª… | ì •ê·œì‹ (ì´ë¦„ íŒ¨í„´) |
| `target_price` | integer | ëª©í‘œê°€ (ì›) | ì •ê·œì‹ (ìˆ«ì + "ì›" íŒ¨í„´) |
| `rating` | string | íˆ¬ìì˜ê²¬ (ë§¤ìˆ˜/ì¤‘ë¦½/ë§¤ë„ ë“±) | í‚¤ì›Œë“œ ë§¤ì¹­ |
| `report_type` | string | ë¦¬í¬íŠ¸ ìœ í˜• | í‚¤ì›Œë“œ ë¶„ë¥˜ |
| `source_file` | string | ì›ë³¸ PDF íŒŒì¼ëª… | íŒŒì¼ ê²½ë¡œì—ì„œ ì¶”ì¶œ |

### 3.2 ì •ê·œì‹ ê¸°ë°˜ ì¶”ì¶œ

ë¦¬í¬íŠ¸ ì²« 1~2 í˜ì´ì§€ì— í•µì‹¬ ë©”íƒ€ë°ì´í„°ê°€ ì§‘ì¤‘ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, íŒŒì‹± ê²°ê³¼ì˜ ìƒìœ„ ë¶€ë¶„ì„ ìš°ì„  ë¶„ì„í•œë‹¤.

```python
import re
from datetime import datetime

def extract_date(text: str) -> str | None:
    """ë¦¬í¬íŠ¸ ë°œí–‰ì¼ ì¶”ì¶œ"""
    patterns = [
        r"(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})",       # 2026.02.10, 2026-02-10
        r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼",         # 2026ë…„ 2ì›” 10ì¼
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            y, m, d = match.groups()
            return f"{y}-{int(m):02d}-{int(d):02d}"
    return None

def extract_target_price(text: str) -> int | None:
    """ëª©í‘œê°€ ì¶”ì¶œ"""
    patterns = [
        r"ëª©í‘œì£¼?ê°€\s*[:\s]*([0-9,]+)\s*ì›",
        r"Target\s*Price\s*[:\s]*([0-9,]+)",
        r"TP\s*[:\s]*([0-9,]+)\s*ì›",
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            price_str = match.group(1).replace(",", "")
            return int(price_str)
    return None

RATING_KEYWORDS = {
    "ë§¤ìˆ˜": ["ë§¤ìˆ˜", "Buy", "BUY", "Overweight", "ë¹„ì¤‘í™•ëŒ€"],
    "ì¤‘ë¦½": ["ì¤‘ë¦½", "Hold", "HOLD", "Neutral", "ì‹œì¥ìˆ˜ìµë¥ "],
    "ë§¤ë„": ["ë§¤ë„", "Sell", "SELL", "Underweight", "ë¹„ì¤‘ì¶•ì†Œ"],
}

def extract_rating(text: str) -> str | None:
    """íˆ¬ìì˜ê²¬ ì¶”ì¶œ"""
    # íˆ¬ìì˜ê²¬ ê´€ë ¨ ë¬¸ë§¥ ê·¼ì²˜ì—ì„œ íƒìƒ‰
    context_match = re.search(
        r"(?:íˆ¬ìì˜ê²¬|íˆ¬ìë“±ê¸‰|Rating|Recommendation)[:\s]*(\S+)",
        text
    )
    if context_match:
        value = context_match.group(1)
        for rating, keywords in RATING_KEYWORDS.items():
            if any(kw in value for kw in keywords):
                return rating
    return None
```

### 3.3 LLM ê¸°ë°˜ ì¶”ì¶œ (ë³´ì¡°)

ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œì´ ì–´ë ¤ìš´ ê²½ìš°(ë¹„ì •í˜• ë ˆì´ì•„ì›ƒ, íŠ¹ì´í•œ í‘œê¸°ë²•), LLMì„ ë³´ì¡°ì ìœ¼ë¡œ í™œìš©í•œë‹¤.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

METADATA_EXTRACTION_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ì˜ ì²« í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "company_name": "ì¢…ëª©ëª…",
    "ticker": "ì¢…ëª©ì½”ë“œ(6ìë¦¬)",
    "date": "YYYY-MM-DD",
    "broker": "ì¦ê¶Œì‚¬ëª…",
    "analyst": "ì• ë„ë¦¬ìŠ¤íŠ¸ëª…",
    "target_price": ìˆ«ì(ì›),
    "rating": "ë§¤ìˆ˜/ì¤‘ë¦½/ë§¤ë„ ì¤‘ í•˜ë‚˜",
    "report_type": "ì‹¤ì ë¶„ì„/ê¸°ì—…ë¶„ì„/ì—…ì¢…ë¶„ì„ ì¤‘ í•˜ë‚˜"
}}
ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” í•„ë“œëŠ” nullë¡œ í‘œì‹œí•˜ì„¸ìš”."""),
    ("user", "{first_page_text}")
])

async def extract_metadata_with_llm(first_page_text: str) -> dict:
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    chain = METADATA_EXTRACTION_PROMPT | llm
    result = await chain.ainvoke({"first_page_text": first_page_text})
    return parse_json_response(result.content)
```

**ì¶”ì¶œ ì „ëµ ìš°ì„ ìˆœìœ„:**
1. ì •ê·œì‹ ê¸°ë°˜ ì¶”ì¶œ ì‹œë„ (ë¹ ë¥´ê³  ë¹„ìš© ì—†ìŒ)
2. ì¶”ì¶œ ì‹¤íŒ¨ í•„ë“œê°€ ìˆìœ¼ë©´ LLM ê¸°ë°˜ ì¶”ì¶œë¡œ ë³´ì™„ (gpt-4o-mini ì‚¬ìš©ìœ¼ë¡œ ë¹„ìš© ìµœì†Œí™”)
3. ìµœì¢… ê²°ê³¼ ê²€ì¦ (í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ì‹œ ê²½ê³  ë¡œê·¸)

---

## 4. Embedding ì„¤ê³„

### 4.1 ëª¨ë¸ ì„ íƒ: text-embedding-3-small

| ê¸°ì¤€ | text-embedding-3-small | text-embedding-3-large |
|------|------------------------|------------------------|
| ì°¨ì› ìˆ˜ | 1,536 (ê¸°ë³¸) | 3,072 (ê¸°ë³¸) |
| ë¹„ìš© | $0.02 / 1M tokens | $0.13 / 1M tokens |
| MTEB ë²¤ì¹˜ë§ˆí¬ | 62.3% | 64.6% |
| í•œêµ­ì–´ ì„±ëŠ¥ | ì–‘í˜¸ (ë‹¤êµ­ì–´ í•™ìŠµ) | ì•½ê°„ ìš°ìˆ˜ |
| ì €ì¥ ê³µê°„ | ìƒëŒ€ì  ì‘ìŒ | ì•½ 2ë°° |

**ì„ íƒ ê·¼ê±°:**
- ë¹„ìš©ì´ 6.5ë°° ì°¨ì´ë‚˜ëŠ” ë°˜ë©´ ì„±ëŠ¥ ì°¨ì´ëŠ” ì•½ 2%pë¡œ ë¯¸ë¯¸
- ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ëŠ” ë„ë©”ì¸ì´ í•œì •ì ì´ë¯€ë¡œ small ëª¨ë¸ë¡œë„ ì¶©ë¶„í•œ êµ¬ë¶„ë ¥ í™•ë³´
- ChromaDB ë¡œì»¬ í™˜ê²½ì—ì„œ ì €ì¥ ê³µê°„ ë° ê²€ìƒ‰ ì†ë„ ì¸¡ë©´ì—ì„œ ìœ ë¦¬
- í•™ìŠµ/í”„ë¡œí† íƒ€ì´í•‘ ë‹¨ê³„ì—ì„œ ë¹„ìš© íš¨ìœ¨ì´ ì¤‘ìš”

### 4.2 ì°¨ì› ì„¤ì •

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    dimensions=1536,  # ê¸°ë³¸ê°’ ìœ ì§€. ì°¨ì› ì¶•ì†Œ(ì˜ˆ: 512) ì‹œ í•œêµ­ì–´ ì„±ëŠ¥ ì €í•˜ ìš°ë ¤
)
```

- `text-embedding-3-small`ì€ Matryoshka Representation Learningì„ ì§€ì›í•˜ì—¬ ì°¨ì› ì¶•ì†Œê°€ ê°€ëŠ¥í•˜ë‚˜, í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë¯¸ì„¸í•œ ì˜ë¯¸ ì°¨ì´(ì¢…ëª©ëª…, ì¦ê¶Œ ìš©ì–´ ë“±)ë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´ ê¸°ë³¸ 1,536 ì°¨ì›ì„ ìœ ì§€í•œë‹¤.

### 4.3 í•œêµ­ì–´ ì„±ëŠ¥ íŠ¹ì„±

- OpenAI embedding ëª¨ë¸ì€ ë‹¤êµ­ì–´ ì½”í¼ìŠ¤ë¡œ í•™ìŠµë˜ì–´ í•œêµ­ì–´ ì§€ì›
- í•œêµ­ì–´ ì¦ê¶Œ ìš©ì–´(PER, PBR, EPS ë“±)ëŠ” ì˜ë¬¸ ì•½ì–´ê°€ ê·¸ëŒ€ë¡œ ì‚¬ìš©ë˜ì–´ embedding í’ˆì§ˆì— ìœ ë¦¬
- ì¢…ëª©ëª…ì€ í•œê¸€ ê³ ìœ ëª…ì‚¬ì´ë¯€ë¡œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ë³´ì™„ (embedding ìœ ì‚¬ë„ë§Œìœ¼ë¡œ ì¢…ëª© êµ¬ë¶„ í•œê³„)

---

## 5. ChromaDB ì ì¬

### 5.1 ì»¬ë ‰ì…˜ êµ¬ì„±

ë‹¨ì¼ ì»¬ë ‰ì…˜ìœ¼ë¡œ êµ¬ì„±í•˜ê³ , ë©”íƒ€ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ì œì–´í•œë‹¤.

```python
import chromadb
from langchain_chroma import Chroma

COLLECTION_NAME = "securities_reports"
PERSIST_DIR = "./data/chromadb"

vectorstore = Chroma(
    collection_name=COLLECTION_NAME,
    embedding_function=embeddings,
    persist_directory=PERSIST_DIR,
    collection_metadata={
        "hnsw:space": "cosine",  # distance metric
        "hnsw:M": 16,            # HNSW ê·¸ë˜í”„ ì—°ê²° ìˆ˜ (ê¸°ë³¸ê°’)
        "hnsw:construction_ef": 100,  # ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œ íƒìƒ‰ ë²”ìœ„
    },
)
```

**ë‹¨ì¼ ì»¬ë ‰ì…˜ ì„ íƒ ê·¼ê±°:**
- ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ëŠ” ë™ì¼ ë„ë©”ì¸ì´ë¯€ë¡œ ì»¬ë ‰ì…˜ ë¶„ë¦¬ ë¶ˆí•„ìš”
- ë©”íƒ€ë°ì´í„° í•„í„°(`company_name`, `broker`, `date`)ë¡œ ì„¸ë°€í•œ ë²”ìœ„ ì§€ì • ê°€ëŠ¥
- SelfQueryRetrieverê°€ ë‹¨ì¼ ì»¬ë ‰ì…˜ ê¸°ì¤€ìœ¼ë¡œ ë™ì‘

### 5.2 Distance Metric: Cosine Similarity

| Metric | íŠ¹ì„± | ì í•©ì„± |
|--------|------|--------|
| Cosine | ë°©í–¥ ê¸°ë°˜ ìœ ì‚¬ë„, ë²¡í„° í¬ê¸° ë¬´ê´€ | í…ìŠ¤íŠ¸ ì˜ë¯¸ ìœ ì‚¬ë„ì— ì í•© |
| L2 (Euclidean) | ê±°ë¦¬ ê¸°ë°˜, ë²¡í„° í¬ê¸°ì— ë¯¼ê° | í´ëŸ¬ìŠ¤í„°ë§ì— ì í•© |
| Inner Product | ë‚´ì  ê¸°ë°˜, ì •ê·œí™” í•„ìš” | ì¶”ì²œ ì‹œìŠ¤í…œì— ì í•© |

**Cosine ì„ íƒ ì´ìœ :**
- í…ìŠ¤íŠ¸ ì„ë² ë”©ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¹„êµì— ê°€ì¥ ë„ë¦¬ ì‚¬ìš©
- OpenAI embeddingì€ ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆì–´ cosineê³¼ inner product ê²°ê³¼ê°€ ë™ì¼í•˜ì§€ë§Œ, ì˜ë¯¸ì  ëª…í™•ì„±ì„ ìœ„í•´ cosine ì‚¬ìš©
- ë¬¸ì„œ ê¸¸ì´ì— ë”°ë¥¸ ë²¡í„° í¬ê¸° ì°¨ì´ì— ì˜í–¥ë°›ì§€ ì•ŠìŒ

### 5.3 ë©”íƒ€ë°ì´í„° ì¸ë±ì‹±

ChromaDBëŠ” ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ ìë™ìœ¼ë¡œ ì§€ì›í•˜ë©°, ë³„ë„ ì¸ë±ìŠ¤ ì„¤ì • ì—†ì´ `where` ì ˆë¡œ í•„í„°ë§ ê°€ëŠ¥í•˜ë‹¤.

```python
# ì ì¬ ì‹œ ë©”íƒ€ë°ì´í„° í¬í•¨
vectorstore.add_documents(
    documents=chunks,
    # ê° Documentì— metadata dictê°€ í¬í•¨ë¨:
    # {
    #     "ticker": "005930",
    #     "company_name": "ì‚¼ì„±ì „ì",
    #     "date": "2026-02-10",
    #     "broker": "ë¯¸ë˜ì—ì…‹ì¦ê¶Œ",
    #     "analyst": "í™ê¸¸ë™",
    #     "target_price": 85000,
    #     "rating": "ë§¤ìˆ˜",
    #     "report_type": "ì‹¤ì ë¶„ì„",
    #     "source_file": "mirae_samsung_20260210.pdf",
    #     "chunk_type": "text",  # "text" | "table"
    #     "section_header": "ì‹¤ì  ë¶„ì„",  # MarkdownHeaderTextSplitterì—ì„œ ì¶”ì¶œ
    # }
)
```

**ë©”íƒ€ë°ì´í„° í•„ë“œë³„ íƒ€ì…:**
- `string` íƒ€ì…: `ticker`, `company_name`, `date`, `broker`, `analyst`, `rating`, `report_type`, `source_file`, `chunk_type`, `section_header`
- `integer` íƒ€ì…: `target_price`

---

## 6. Retriever ì„¤ê³„

### 6.1 SelfQueryRetriever êµ¬ì„±

SelfQueryRetrieverëŠ” ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì—ì„œ ì˜ë¯¸ì  ì¿¼ë¦¬ì™€ ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ìë™ìœ¼ë¡œ ë¶„ë¦¬í•œë‹¤.

**ì˜ˆì‹œ:**
- ì§ˆë¬¸: "ì‚¼ì„±ì „ì ìµœê·¼ ëª©í‘œê°€ ì•Œë ¤ì¤˜"
- ë¶„ë¦¬ ê²°ê³¼:
  - Semantic query: "ëª©í‘œê°€"
  - Metadata filter: `company_name == "ì‚¼ì„±ì „ì"`, sort by `date` desc

```python
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.schema import AttributeInfo

metadata_field_info = [
    AttributeInfo(
        name="company_name",
        description="ì¢…ëª©ëª…. ì˜ˆ: ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤, LGì—ë„ˆì§€ì†”ë£¨ì…˜, ë„¤ì´ë²„, ì¹´ì¹´ì˜¤",
        type="string",
    ),
    AttributeInfo(
        name="ticker",
        description="ì¢…ëª©ì½”ë“œ(6ìë¦¬ ìˆ«ì). ì˜ˆ: 005930(ì‚¼ì„±ì „ì), 000660(SKí•˜ì´ë‹‰ìŠ¤)",
        type="string",
    ),
    AttributeInfo(
        name="date",
        description="ë¦¬í¬íŠ¸ ë°œí–‰ì¼. í˜•ì‹: YYYY-MM-DD. ì˜ˆ: 2026-02-10",
        type="string",
    ),
    AttributeInfo(
        name="broker",
        description="ë¦¬í¬íŠ¸ë¥¼ ë°œí–‰í•œ ì¦ê¶Œì‚¬ëª…. ì˜ˆ: ë¯¸ë˜ì—ì…‹ì¦ê¶Œ, í•œêµ­íˆ¬ìì¦ê¶Œ, NHíˆ¬ìì¦ê¶Œ, ì‚¼ì„±ì¦ê¶Œ, KBì¦ê¶Œ",
        type="string",
    ),
    AttributeInfo(
        name="analyst",
        description="ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•œ ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë¦„",
        type="string",
    ),
    AttributeInfo(
        name="report_type",
        description="ë¦¬í¬íŠ¸ ìœ í˜•. ê°€ëŠ¥í•œ ê°’: ì‹¤ì ë¶„ì„, ê¸°ì—…ë¶„ì„, ì—…ì¢…ë¶„ì„",
        type="string",
    ),
    AttributeInfo(
        name="rating",
        description="íˆ¬ìì˜ê²¬. ê°€ëŠ¥í•œ ê°’: ë§¤ìˆ˜, ì¤‘ë¦½, ë§¤ë„",
        type="string",
    ),
    AttributeInfo(
        name="target_price",
        description="ëª©í‘œ ì£¼ê°€(ì›). ì •ìˆ˜ê°’",
        type="integer",
    ),
    AttributeInfo(
        name="chunk_type",
        description="ì²­í¬ ìœ í˜•. ê°€ëŠ¥í•œ ê°’: text, table",
        type="string",
    ),
]

DOCUMENT_CONTENT_DESCRIPTION = (
    "ì¦ê¶Œì‚¬ ì• ë„ë¦¬ìŠ¤íŠ¸ê°€ ì‘ì„±í•œ ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸. "
    "ì‹¤ì  ë¶„ì„, íˆ¬ì ì˜ê²¬, ëª©í‘œ ì£¼ê°€, ë°¸ë¥˜ì—ì´ì…˜, ì—…ì¢… ì „ë§ ë“±ì˜ ë‚´ìš©ì„ í¬í•¨."
)

retriever = SelfQueryRetriever.from_llm(
    llm=ChatOpenAI(model="gpt-4o-mini", temperature=0),
    vectorstore=vectorstore,
    document_contents=DOCUMENT_CONTENT_DESCRIPTION,
    metadata_field_info=metadata_field_info,
    enable_limit=True,
    search_type="similarity_score_threshold",
    search_kwargs={
        "k": 5,
        "score_threshold": 0.3,
    },
)
```

### 6.2 ê²€ìƒ‰ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ê·¼ê±° |
|----------|-----|------|
| `k` | 5 | ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë‹µë³€ì— 3~5ê°œ ì¶œì²˜ê°€ ì ì •. ë„ˆë¬´ ë§ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì¦ê°€ |
| `score_threshold` | 0.3 | cosine similarity ê¸°ì¤€. ì¦ê¶Œ ë„ë©”ì¸ íŠ¹ì„±ìƒ ê´€ë ¨ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë¯€ë¡œ ë‚®ì€ thresholdë¡œ recall í™•ë³´ í›„ LLMì´ í•„í„°ë§ |
| `search_type` | `similarity_score_threshold` | ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œê°€ í˜¼ì…ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ threshold ì ìš© |

### 6.3 Fallback ì „ëµ

SelfQueryRetrieverê°€ ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ê³¼ë„í•˜ê²Œ ì ìš©í•˜ì—¬ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œë‹¤.

```python
def retrieve_with_fallback(query: str) -> list:
    # 1ì°¨: SelfQueryRetriever (ë©”íƒ€ë°ì´í„° í•„í„° + ìœ ì‚¬ë„)
    results = retriever.invoke(query)

    if not results:
        # 2ì°¨: ë©”íƒ€ë°ì´í„° í•„í„° ì—†ì´ ìˆœìˆ˜ ìœ ì‚¬ë„ ê²€ìƒ‰
        results = vectorstore.similarity_search_with_score(
            query, k=5
        )
        results = [doc for doc, score in results if score >= 0.3]

    return results
```

---

## 7. QA Chain ì„¤ê³„

### 7.1 í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

#### System Prompt

```python
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì œê³µëœ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.

## ë‹µë³€ ê·œì¹™

1. **ì¶œì²˜ ê¸°ë°˜ ë‹µë³€**: ë°˜ë“œì‹œ ì œê³µëœ ë¦¬í¬íŠ¸ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì œê³µë˜ì§€ ì•Šì€ ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ê±°ë‚˜ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
2. **ì¶œì²˜ í‘œì‹œ**: ë‹µë³€ ëì— ì°¸ê³ í•œ ë¦¬í¬íŠ¸ì˜ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”.
3. **í…Œì´ë¸” í™œìš©**: ì‹¤ì  ë°ì´í„°, ë°¸ë¥˜ì—ì´ì…˜ ë“± ìˆ«ì ì •ë³´ëŠ” ê°€ëŠ¥í•˜ë©´ í…Œì´ë¸” í˜•íƒœë¡œ ì •ë¦¬í•˜ì„¸ìš”.
4. **ë³µìˆ˜ ë¦¬í¬íŠ¸ ì¢…í•©**: ì—¬ëŸ¬ ì¦ê¶Œì‚¬ì˜ ë¦¬í¬íŠ¸ê°€ ìˆì„ ê²½ìš°, ê° ì¦ê¶Œì‚¬ì˜ ì˜ê²¬ì„ ë¹„êµí•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
5. **ë‹µë³€ ë¶ˆê°€ ì‹œ**: ê´€ë ¨ ë¦¬í¬íŠ¸ê°€ ì—†ê±°ë‚˜ ë‹µë³€í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, "ì œê³µëœ ë¦¬í¬íŠ¸ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•íˆ ë°íˆì„¸ìš”.
6. **í•œêµ­ì–´ ë‹µë³€**: í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. ìˆ«ìì™€ ê¸°ìˆ  ìš©ì–´ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

## ì¶œì²˜ í‘œì‹œ í˜•ì‹

ë‹µë³€ ëì— ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”:
---
ğŸ“Š ì¶œì²˜:
- [ì¦ê¶Œì‚¬ëª…] ì• ë„ë¦¬ìŠ¤íŠ¸ëª…, "ì¢…ëª©ëª… ë¦¬í¬íŠ¸" (YYYY.MM.DD)
"""
```

#### User Prompt

```python
USER_PROMPT = """ë‹¤ìŒì€ ê´€ë ¨ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë‚´ìš©ì…ë‹ˆë‹¤:

{context}

---
ì‚¬ìš©ì ì§ˆë¬¸: {question}

ìœ„ ë¦¬í¬íŠ¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
```

### 7.2 Chain êµ¬ì„±

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

def format_docs(docs: list) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ í¬ë§·"""
    formatted = []
    for i, doc in enumerate(docs, 1):
        meta = doc.metadata
        header = (
            f"[ì¶œì²˜ {i}] {meta.get('broker', 'ì•Œ ìˆ˜ ì—†ìŒ')} | "
            f"{meta.get('analyst', 'ì•Œ ìˆ˜ ì—†ìŒ')} | "
            f"{meta.get('company_name', 'ì•Œ ìˆ˜ ì—†ìŒ')} | "
            f"{meta.get('date', 'ë‚ ì§œ ë¯¸ìƒ')}"
        )
        formatted.append(f"{header}\n{doc.page_content}")
    return "\n\n---\n\n".join(formatted)

prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    ("user", USER_PROMPT),
])

llm = ChatOpenAI(
    model="gpt-4o-mini",   # ê°œë°œ ë‹¨ê³„
    # model="gpt-4o",       # ìš´ì˜ ë‹¨ê³„
    temperature=0,
)

rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough(),
    }
    | prompt
    | llm
    | StrOutputParser()
)
```

### 7.3 ë‹µë³€ ë¶ˆê°€ ì²˜ë¦¬

ë‹µë³€ ë¶ˆê°€ ìƒí™©ì€ ë‘ ê°€ì§€ë¡œ êµ¬ë¶„í•œë‹¤:

1. **ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ** (retrieverê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
   - "ê´€ë ¨ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ëª©ëª…ì´ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

2. **ê²€ìƒ‰ ê²°ê³¼ëŠ” ìˆì§€ë§Œ ì§ˆë¬¸ê³¼ ë¬´ê´€** (LLMì´ íŒë‹¨)
   - System promptì˜ ê·œì¹™ 5ì— ì˜í•´ LLMì´ "ì œê³µëœ ë¦¬í¬íŠ¸ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." ì‘ë‹µ

```python
async def answer_question(question: str) -> str:
    docs = retriever.invoke(question)

    if not docs:
        return (
            "ê´€ë ¨ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            "ì¢…ëª©ëª…ì´ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n\n"
            "ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸:\n"
            "- ì‚¼ì„±ì „ì ëª©í‘œê°€ ì•Œë ¤ì¤˜\n"
            "- SKí•˜ì´ë‹‰ìŠ¤ ìµœê·¼ ì‹¤ì  ìš”ì•½\n"
            "- ë°˜ë„ì²´ ì—…ì¢… ì „ë§"
        )

    return await rag_chain.ainvoke(question)
```

---

## 8. í’ˆì§ˆ í‰ê°€

### 8.1 Retrieval ì •í™•ë„ ì¸¡ì •

#### Hit Rate (ì ì¤‘ë¥ )

kê°œì˜ ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ì •ë‹µ ë¬¸ì„œê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ì ì¤‘ìœ¼ë¡œ ê°„ì£¼í•œë‹¤.

```
Hit Rate@k = (ì •ë‹µì´ í¬í•¨ëœ ì§ˆë¬¸ ìˆ˜) / (ì „ì²´ ì§ˆë¬¸ ìˆ˜)
```

#### Mean Reciprocal Rank (MRR)

ì •ë‹µ ë¬¸ì„œê°€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëª‡ ë²ˆì§¸ì— ë‚˜íƒ€ë‚˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•œë‹¤.

```
MRR = (1/N) * Î£ (1 / rank_i)
```

### 8.2 ì§ˆë¬¸-ì •ë‹µ ìŒ í‰ê°€ í”„ë ˆì„ì›Œí¬

#### í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì„±

ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë°˜ì˜í•œ ì§ˆë¬¸-ì •ë‹µ ìŒì„ ìˆ˜ë™ìœ¼ë¡œ êµ¬ì„±í•œë‹¤.

```python
EVAL_DATASET = [
    {
        "question": "ì‚¼ì„±ì „ì ëª©í‘œê°€ ì»¨ì„¼ì„œìŠ¤ ì•Œë ¤ì¤˜",
        "expected_answer_contains": ["ëª©í‘œê°€", "ì›"],
        "expected_sources": {"company_name": "ì‚¼ì„±ì „ì"},
        "category": "factual",
    },
    {
        "question": "SKí•˜ì´ë‹‰ìŠ¤ ìµœê·¼ ì‹¤ì  ìš”ì•½í•´ì¤˜",
        "expected_answer_contains": ["ë§¤ì¶œ", "ì˜ì—…ì´ìµ"],
        "expected_sources": {"company_name": "SKí•˜ì´ë‹‰ìŠ¤"},
        "category": "summary",
    },
    {
        "question": "ë°˜ë„ì²´ ì—…ì¢… ì „ë§ ë¦¬í¬íŠ¸ ì¤‘ ê°€ì¥ ê¸ì •ì ì¸ ì˜ê²¬ì€?",
        "expected_answer_contains": ["ë§¤ìˆ˜", "ê¸ì •"],
        "expected_sources": {"report_type": "ì—…ì¢…ë¶„ì„"},
        "category": "comparison",
    },
    {
        "question": "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?",
        "expected_answer_contains": ["ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
        "expected_sources": None,
        "category": "out_of_scope",
    },
]
```

#### í‰ê°€ ì¹´í…Œê³ ë¦¬

| ì¹´í…Œê³ ë¦¬ | ì„¤ëª… | í‰ê°€ ê¸°ì¤€ |
|----------|------|-----------|
| `factual` | íŠ¹ì • ìˆ˜ì¹˜/íŒ©íŠ¸ ì§ˆì˜ | ì •í™•í•œ ìˆ˜ì¹˜ í¬í•¨ ì—¬ë¶€ |
| `summary` | ìš”ì•½ ì§ˆì˜ | í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ |
| `comparison` | ë¹„êµ ì§ˆì˜ | ë³µìˆ˜ ì¶œì²˜ ì°¸ì¡° ì—¬ë¶€ |
| `table` | í…Œì´ë¸” ë°ì´í„° ì§ˆì˜ | í…Œì´ë¸” ì²­í¬ ê²€ìƒ‰ ì—¬ë¶€ |
| `out_of_scope` | ë²”ìœ„ ì™¸ ì§ˆë¬¸ | ì ì ˆí•œ ê±°ì ˆ ì‘ë‹µ |

#### í‰ê°€ ì‹¤í–‰

```python
async def evaluate_rag(eval_dataset: list) -> dict:
    results = {"total": len(eval_dataset), "passed": 0, "failed": 0, "details": []}

    for item in eval_dataset:
        answer = await answer_question(item["question"])

        # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
        keywords_found = all(
            kw in answer for kw in item["expected_answer_contains"]
        )

        result = {
            "question": item["question"],
            "category": item["category"],
            "passed": keywords_found,
            "answer_preview": answer[:200],
        }

        if keywords_found:
            results["passed"] += 1
        else:
            results["failed"] += 1

        results["details"].append(result)

    results["pass_rate"] = results["passed"] / results["total"]
    return results
```

### 8.3 í‰ê°€ ëª©í‘œ ì§€í‘œ

| ì§€í‘œ | ëª©í‘œê°’ | ì„¤ëª… |
|------|--------|------|
| Hit Rate@5 | >= 0.8 | 5ê°œ ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ì •ë‹µ í¬í•¨ |
| MRR | >= 0.6 | ì •ë‹µì´ ìƒìœ„ì— ìœ„ì¹˜ |
| í‚¤ì›Œë“œ Pass Rate | >= 0.7 | ë‹µë³€ ë‚´ ê¸°ëŒ€ í‚¤ì›Œë“œ í¬í•¨ |
| Out-of-scope ì •í™•ë„ | >= 0.9 | ë²”ìœ„ ì™¸ ì§ˆë¬¸ ì •í™• ê±°ì ˆ |

### 8.4 ë°˜ë³µ ê°œì„  í”„ë¡œì„¸ìŠ¤

```
í‰ê°€ ì‹¤í–‰ â†’ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„ â†’ ì›ì¸ ë¶„ë¥˜ â†’ ê°œì„  ì ìš© â†’ ì¬í‰ê°€
```

**ì›ì¸ ë¶„ë¥˜ ë° ëŒ€ì‘:**

| ì›ì¸ | ëŒ€ì‘ |
|------|------|
| ê´€ë ¨ ì²­í¬ê°€ ê²€ìƒ‰ë˜ì§€ ì•ŠìŒ | ì²­í‚¹ ì „ëµ ì¡°ì •, chunk_size/overlap ë³€ê²½ |
| ê´€ë ¨ ì²­í¬ê°€ ê²€ìƒ‰ë˜ì—ˆìœ¼ë‚˜ ìˆœìœ„ê°€ ë‚®ìŒ | embedding ëª¨ë¸ ë³€ê²½ ë˜ëŠ” query ë¦¬í¬ë§· |
| ê²€ìƒ‰ì€ ì˜ ë˜ì—ˆìœ¼ë‚˜ ë‹µë³€ í’ˆì§ˆ ì €í•˜ | QA prompt ê°œì„ , LLM ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ |
| ë©”íƒ€ë°ì´í„° í•„í„°ê°€ ì˜ëª» ì ìš©ë¨ | SelfQueryRetrieverì˜ AttributeInfo ì„¤ëª… ë³´ì™„ |

---

## ë¶€ë¡: ì „ì²´ íŒŒì´í”„ë¼ì¸ ë°ì´í„° íë¦„

```
PDF íŒŒì¼
  â”‚
  â–¼
[1. Upstage Document Parse API]
  â”‚  output: Markdown í…ìŠ¤íŠ¸
  â”‚
  â–¼
[2. ë””ìŠ¤í´ë ˆì´ë¨¸ í•„í„°ë§]
  â”‚  output: ì •ì œëœ Markdown
  â”‚
  â–¼
[3. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  output: metadata dict         â”‚
  â”‚                                â”‚
  â–¼                                â”‚
[4. í…Œì´ë¸”/í…ìŠ¤íŠ¸ ë¶„ë¦¬]              â”‚
  â”‚                                â”‚
  â”œâ”€â”€ í…Œì´ë¸” ì²­í¬ (í†µì§¸ ë³´ì¡´)        â”‚
  â”‚                                â”‚
  â””â”€â”€ í…ìŠ¤íŠ¸ ì²­í¬                    â”‚
       â”‚  MarkdownHeaderTextSplitter â”‚
       â”‚  + RecursiveCharacterText   â”‚
       â”‚    Splitter                 â”‚
       â–¼                            â”‚
[5. Embedding]                      â”‚
  â”‚  text-embedding-3-small         â”‚
  â”‚  1,536 dims                     â”‚
  â–¼                                 â”‚
[6. ChromaDB ì ì¬]  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚  collection: securities_reports
  â”‚  metric: cosine
  â”‚
  â–¼
[7. SelfQueryRetriever]
  â”‚  ë©”íƒ€ë°ì´í„° í•„í„° + ìœ ì‚¬ë„ ê²€ìƒ‰
  â”‚  k=5, score_threshold=0.3
  â”‚
  â–¼
[8. QA Chain]
  â”‚  System prompt + Context + Question
  â”‚  gpt-4o-mini (ê°œë°œ) / gpt-4o (ìš´ì˜)
  â”‚
  â–¼
ë‹µë³€ + ì¶œì²˜ í‘œì‹œ
```
